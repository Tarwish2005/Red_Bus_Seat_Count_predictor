

# Red Bus Final Seat Count Prediction

## 1\. Overview

This project aims to predict the `final_seatcount` for bus routes on a given date of journey (DOJ). The problem is approached as a time-series regression task. The solution leverages historical transaction data to build sophisticated features like lags and rolling window statistics. The final prediction is generated by an ensemble of LightGBM models trained using 10-fold cross-validation.

The core idea is that booking patterns in the days leading up to a journey (specifically, 15 to 30 days before) are highly predictive of the final number of seats sold.

## 2\. Dataset

The prediction model is built using three datasets:

  * `train.csv`: Contains the training data with the target variable.
      * `doj`: Date of Journey.
      * `srcid`: Source ID of the route.
      * `destid`: Destination ID of the route.
      * `final_seatcount`: The total number of seats sold (the target variable).
  * `test.csv`: Contains the test data for which predictions are required. It has the same structure as `train.csv` but without the `final_seatcount`.
  * `transactions.csv`: Provides detailed historical data about cumulative seat bookings and searches for each route.
      * `doj`: Date of Journey.
      * `doi`: Date of Inquiry.
      * `dbd`: Days Before Departure (`doj` - `doi`).
      * `cumsum_seatcount`: Cumulative seats booked up to that `doi`.
      * `cumsum_searchcount`: Cumulative searches made up to that `doi`.
      * Other metadata like region and tier for source/destination.

## 3\. Methodology

The solution follows a structured machine learning pipeline involving extensive feature engineering, model training, and ensembling.

### 3.1. Data Preprocessing

1.  **Data Merging**: The primary features are derived from the state of bookings 15 days before departure (`dbd=15`). The `train.csv` and `test.csv` files are merged with the `transactions.csv` data where `dbd` is 15.
2.  **Log Transformation**: To handle skewed distributions and stabilize variance, a log transformation (`np.log1p`) is applied to all cumulative count features (`cumsum_seatcount`, `cumsum_searchcount`) and the engineered lag features. The target variable (`final_seatcount`) is also log-transformed with an offset (`np.log1p(y + 475)`) to improve model performance.

### 3.2. Feature Engineering

This is the most critical part of the project. The features are designed to capture the time-series nature of bus bookings.

1.  **Lag Features**:
      * Historical data for `cumsum_seatcount` and `cumsum_searchcount` from 16 to 30 days before departure (`dbd` \> 15 and \<= 30) are pivoted to create lag features for each day.
2.  **Rolling Window Statistics**:
      * To capture trends, rolling statistics (mean, standard deviation, and difference) are calculated over a short-term window (lags 16-20). This helps the model understand the recent booking momentum.
3.  **Cyclical Date/Time Features**:
      * Date columns (`doj`, `doi`) are broken down into components like month, day, and weekday.
      * These components are then transformed into cyclical features using sine and cosine functions (`np.sin`, `np.cos`) to help the model understand the cyclical nature of days, months, and weeks (e.g., booking patterns on weekends vs. weekdays).
4.  **Categorical Feature Encoding**:
      * Categorical features like `srcid_region`, `destid_region`, `srcid_tier`, and `destid_tier` are mapped to numerical integers.
5.  **Interaction Features**:
      * A `seatcount_searchcount_ratio` is created to capture the conversion rate of searches to bookings.
      * A unique `route_path_encoded` feature is generated by combining source and destination IDs.

### 3.3. Modeling and Training

1.  **Model Selection**: Several gradient boosting models were evaluated, including **XGBoost**, **CatBoost**, and **LightGBM**. LightGBM was chosen for the final model due to its performance and speed.
2.  **Hyperparameter Tuning**: The LightGBM model's hyperparameters were optimized using the **Optuna** framework to find the best combination for minimizing the Root Mean Squared Error (RMSE).
3.  **Sample Weighting**: To give more importance to routes with higher demand (and thus higher variance), sample weights are calculated based on the target value (`final_seatcount`). This helps the model focus on predicting high-value targets more accurately.
4.  **Cross-Validation & Ensembling**:
      * A **10-Fold Cross-Validation** strategy is employed on the full training dataset.
      * In each fold, a LightGBM model is trained with the best-found hyperparameters. Each of the 10 models is saved to disk.
      * For final prediction on the test set, predictions from all 10 models are averaged. This ensembling technique makes the final prediction more robust and stable.

### 3.4. Prediction and Submission

1.  The test data undergoes the same feature engineering pipeline as the training data.
2.  The ensemble of 10 LightGBM models is used to predict the log-transformed seat counts.
3.  The predictions are averaged and then inverse-transformed (`np.expm1`) to bring them back to the original scale.
4.  Final predictions are clipped to have a minimum value of 0 and rounded to the nearest integer before being saved to `submission.csv`.

## 4\. How to Run

To replicate the results, follow these steps:

1.  **Clone the repository:**

    ```bash
    git clone <repository-url>
    cd <repository-name>
    ```

2.  **Install dependencies:**
    It's recommended to use a virtual environment.

    ```bash
    pip install -r requirements.txt
    ```

3.  **Place the data:**
    Ensure the data files (`train.csv`, `test.csv`, `transactions.csv`, `solution_format.csv`) are in the root directory of the project.

4.  **Run the Jupyter Notebook:**
    Launch Jupyter Notebook and open `Red_Bus_Prediction.ipynb`. Run all the cells in order. The notebook will train the models, generate predictions, and save the final `submission_m.csv` file.

## 5\. Requirements

The project uses the following major libraries:

  * pandas
  * numpy
  * scikit-learn
  * xgboost
  * lightgbm
  * catboost
  * matplotlib
  * seaborn

A full list of dependencies is available in the `requirements.txt` file.

## 6\. File Structure

```
.
├── Red_Bus_Prediction.ipynb    # Main Jupyter Notebook with all the code
├── train.csv                   # Training data
├── test.csv                    # Test data
├── transactions.csv            # Historical transaction data
├── solution_format.csv         # Sample submission file format
├── submission_m.csv            # Generated submission file
├── requirements.txt            # Python dependencies
└── README.md                   # This file
```
